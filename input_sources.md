1. Protecting Society from AI-Generated Misinformation: A Guide for Ethical AI Use | Analytics Magazine - PubsOnLine, consulté le avril 12, 2025, https://pubsonline.informs.org/do/10.1287/LYTX.2025.01.06/full/
2. AI Misinformation: Here's How to Reduce Your Company's Exposure and Risk | IBM, consulté le avril 12, 2025, https://www.ibm.com/think/insights/ai-misinformation
3. Using Responsible AI to combat misinformation - Trilateral Research, consulté le avril 12, 2025, https://trilateralresearch.com/responsible-ai/using-responsible-ai-to-combat-misinformation
4. AI Misinformation: Concerns and Prevention Methods - GlobalSign, consulté le avril 12, 2025, https://www.globalsign.com/en/blog/ai-misinformation-concerns-and-prevention
5. AI safety solutions mapping: An initiative for advanced AI governance - OECD.AI, consulté le avril 12, 2025, https://oecd.ai/en/wonk/ai-safety-solutions-risk-mapping
6. AI and Privacy: Safeguarding Data in the Age of Artificial Intelligence | DigitalOcean, consulté le avril 12, 2025, https://www.digitalocean.com/resources/articles/ai-and-privacy
7. AI Security: Risks, Frameworks, and Best Practices - Perception Point, consulté le avril 12, 2025, https://perception-point.io/guides/ai-security/ai-security-risks-frameworks-and-best-practices/
8. What Is AI Safety? - IBM, consulté le avril 12, 2025, https://www.ibm.com/think/topics/ai-safety
9. The impact of AI in data privacy protection - Lumenalta, consulté le avril 12, 2025, https://lumenalta.com/insights/the-impact-of-ai-in-data-privacy-protection
10. OWASP AI Security and Privacy Guide, consulté le avril 12, 2025, https://owasp.org/www-project-ai-security-and-privacy-guide/
11. Developing Guardrails for AI Biodesign Tools - Nuclear Threat Initiative (NTI), consulté le avril 12, 2025, https://www.nti.org/analysis/articles/developing-guardrails-for-ai-biodesign-tools/
12. AI Safety Metrics: How to Ensure Secure and Reliable AI Applications, consulté le avril 12, 2025, https://www.galileo.ai/blog/introduction-to-ai-safety
13. The AI Bill of Rights Explained - Wiz, consulté le avril 12, 2025, https://www.wiz.io/academy/ai-bill-of-rights
14. Towards a Standard for Identifying and Managing Bias in Artificial Intelligence - NIST Technical Series Publications, consulté le avril 12, 2025, https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270.pdf
15. Global AI adoption is outpacing risk understanding, warns MIT CSAIL, consulté le avril 12, 2025, https://www.csail.mit.edu/news/global-ai-adoption-outpacing-risk-understanding-warns-mit-csail
16. “I Wonder if my Years of Training and Expertise Will be Devalued by Machines”: Concerns About the Replacement of Medical Professionals by Artificial Intelligence - PMC - PubMed Central, consulté le avril 12, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC11003342/
17. Three Reasons Why AI May Widen Global Inequality | Center For Global Development, consulté le avril 12, 2025, https://www.cgdev.org/blog/three-reasons-why-ai-may-widen-global-inequality
18. From Efficiency Gains to Rebound Effects: The Problem of Jevons' Paradox in AI's Polarized Environmental Debate - arXiv, consulté le avril 12, 2025, https://arxiv.org/html/2501.16548v1
19. Artificial Intelligence, Power and Sustainability · Dataetisk Tænkehandletank - DataEthics.eu, consulté le avril 12, 2025, https://dataethics.eu/artificial-intelligence-power-and-sustainability/
20. Artificial Intelligence Colonialism: Environmental Damage, Labor Exploitation, and Human Rights Crises in the Global South - Project MUSE, consulté le avril 12, 2025, https://muse.jhu.edu/article/950958
21. AI Environmental Risk Mitigation → Term - Sustainability Directory, consulté le avril 12, 2025, https://sustainability-directory.com/term/ai-environmental-risk-mitigation/
22. The Upsurge and Threats of Self-Reproducing AI | Manufacturing.net, consulté le avril 12, 2025, https://www.manufacturing.net/oracle/blog/22935927/the-upsurge-and-threats-of-selfreproducing-ai
23. Risks of AI Self-Replication - RiskNET, consulté le avril 12, 2025, https://www.risknet.de/en/topics/news-details/risks-of-ai-self-replication/
24. Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?, consulté le avril 12, 2025, https://www.lesswrong.com/posts/p5gBcoQeBsvsMShvT/superintelligent-agents-pose-catastrophic-risks-can
25. AI Agents: Potential Risks - Lumenova AI, consulté le avril 12, 2025, https://www.lumenova.ai/blog/ai-agents-potential-risks/
26. Detecting AI fingerprints: A guide to watermarking and beyond - Brookings Institution, consulté le avril 12, 2025, https://www.brookings.edu/articles/detecting-ai-fingerprints-a-guide-to-watermarking-and-beyond/
27. What is Azure AI Content Safety? - Learn Microsoft, consulté le avril 12, 2025, https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview
28. The Race to Detect AI-Generated Content and Tackle Harms | TechPolicy.Press, consulté le avril 12, 2025, https://www.techpolicy.press/the-race-to-detect-aigenerated-content-and-tackle-harms/
29. Statement on Guidance for the University of Pennsylvania Community on Use of Generative Artificial Intelligence | Information Systems & Computing, consulté le avril 12, 2025, https://isc.upenn.edu/security/statement-guidance-university-pennsylvania-community-use-generative-artificial
30. Understanding AI Safety: Principles, Frameworks, and Best Practices - Tigera, consulté le avril 12, 2025, https://www.tigera.io/learn/guides/llm-security/ai-safety/
31. Ensuring AI Is Used Responsibly - Homeland Security, consulté le avril 12, 2025, https://www.dhs.gov/ai/ensuring-ai-is-used-responsibly
32. AI and the Risk of Consumer Harm | Federal Trade Commission, consulté le avril 12, 2025, https://www.ftc.gov/policy/advocacy-research/tech-at-ftc/2025/01/ai-risk-consumer-harm
33. Risk Management Profile for Artificial Intelligence and Human Rights - State Department, consulté le avril 12, 2025, https://2021-2025.state.gov/risk-management-profile-for-ai-and-human-rights/
34. It's hopeful because AI has not devalued creative human labor but increased its, consulté le avril 12, 2025, https://news.ycombinator.com/item?id=43624216
35. Tackling AI, taxation, and the fair distribution of AI's benefits - Equitable Growth, consulté le avril 12, 2025, https://equitablegrowth.org/tackling-ai-taxation-and-the-fair-distribution-of-ais-benefits/
36. The AI Growth and Redistribution Impact Doctrine (AI-GRID) | Emerald Insight, consulté le avril 12, 2025, https://www.emerald.com/insight/content/doi/10.1108/978-1-83662-660-220251008/full/html
37. SALON: AI could widen the wealth gap, experts say - Economic Security Project, consulté le avril 12, 2025, https://economicsecurityproject.org/news/salon-ai-could-widen-the-wealth-gap-experts-say/
38. How we're addressing the gap between AI capabilities and mitigations | AISI Work, consulté le avril 12, 2025, https://www.aisi.gov.uk/work/aisis-research-direction-for-technical-solutions
39. AI Risks and Trustworthiness - NIST AIRC - National Institute of Standards and Technology, consulté le avril 12, 2025, https://airc.nist.gov/airmf-resources/airmf/3-sec-characteristics/
40. We have no science of safe AI - Centre for Future Generations, consulté le avril 12, 2025, https://cfg.eu/we-have-no-science-of-safe-ai/
41. Developing AI Risk Management With the Same Ambition and Urgency as AI Products, consulté le avril 12, 2025, https://carnegieendowment.org/research/2024/12/developing-ai-risk-management-with-the-same-ambition-and-urgency-as-ai-products
42. arxiv.org, consulté le avril 12, 2025, https://arxiv.org/abs/2408.00761
43. AI Safety Strategies Landscape - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/RzsXRbk2ETNqjhsma/ai-safety-strategies-landscape
44. The Crux List — LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/agv26XfXfKfKiKwDm/the-crux-list
45. consulté le janvier 1, 1970, https://www.overcomingbias.com/p/ai-risk-again
46. Comments - AI Risk, Again - by Robin Hanson - Overcoming Bias, consulté le avril 12, 2025, https://www.overcomingbias.com/p/ai-risk-again/comments
47. Robin Hanson AI X-Risk Debate — Highlights and Analysis - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/quj3RniAw7GGJtqeX/robin-hanson-ai-x-risk-debate-highlights-and-analysis
48. Contra Hanson on AI Risk - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/AqQ9qBkroFCKSqydd/contra-hanson-on-ai-risk
49. Can Humanity Survive AI? - Jacobin, consulté le avril 12, 2025, https://jacobin.com/2024/01/can-humanity-survive-ai
50. University of Alberta Alberta Machine Intelligence Institute - Rich Sutton, consulté le avril 12, 2025, http://incompleteideas.net/Talks/waic3.pdf
51. Richard Sutton has done fascinating RL research in the past. Lately, though, he, consulté le avril 12, 2025, https://news.ycombinator.com/item?id=37801918
52. The Path to Human-Level AI by 2040: Rich Sutton's Vision and the Enterprise Imperative | by Chima Team | Apr, 2025 | Medium, consulté le avril 12, 2025, https://medium.com/@chima_ai/the-path-to-human-level-ai-by-2040-rich-suttons-vision-and-the-enterprise-imperative-2cb407543ea8
53. Unexpected Insights From an AI Rock Star | New Trail - University of Alberta, consulté le avril 12, 2025, https://www.ualberta.ca/en/newtrail/people/unexpected-insights-from-an-ai-rock-star.html
54. johnswentworth - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/users/johnswentworth
55. Beware safety-washing — EA Forum - Effective Altruism Forum, consulté le avril 12, 2025, https://forum.effectivealtruism.org/posts/f2qojPr8NaMPo2KJC/beware-safety-washing
56. Safetywashing — LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/xhD6SHAAE9ghKZ9HS/safetywashing
57. The Bitter Lesson for AI Safety Research - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/afiyFjiNyubqvuFHM/the-bitter-lesson-for-ai-safety-research
58. AI Safety at the Frontier: Paper Highlights, July '24 - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/33s54FoKG5g2tbX3f/ai-safety-at-the-frontier-paper-highlights-july-24
59. AISN #45: Center for AI Safety 2024 Year in Review - LessWrong, consulté le avril 12, 2025, https://www.greaterwrong.com/posts/f3LSnkh7JrApcZtLs/aisn-45-center-for-ai-safety-2024-year-in-review?comments=false
60. Thoughts on the impact of RLHF research — AI Alignment Forum, consulté le avril 12, 2025, https://www.alignmentforum.org/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research
61. Technical AI Safety Research Landscape [Slides] - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/x2n7mBLryDXuLwGhx/technical-ai-safety-research-landscape-slides
62. The case for stopping AI safety research - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/vkzmbf4Mve4GNyJaF/the-case-for-stopping-ai-safety-research
63. The Case Against AI Control Research - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/8wBN8cdNAv3c7vt6p/the-case-against-ai-control-research
64. consulté le janvier 1, 1970, https://www.lesswrong.com/posts/N2JcFZ3L7XBe3yGDB/how-the-ai-safety-technical-landscape-has-changed-in-the
65. Shallow review of technical AI safety, 2024 - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/fAW6RXLKTLHC3WXkS/shallow-review-of-technical-ai-safety-2024
66. Chris_Leong comments on How the AI safety technical landscape has changed in the last year, according to some practitioners - LessWrong 2.0 viewer - GreaterWrong, consulté le avril 12, 2025, https://www.greaterwrong.com/posts/vxwdfK7FEuM8Butr6/how-the-ai-safety-technical-landscape-has-changed-in-the/comment/H9Yy8TtbdFgjo7J2n
67. How the AI safety technical landscape has changed in the last year, according to some practitioners - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/vxwdfK7FEuM8Butr6/how-the-ai-safety-technical-landscape-has-changed-in-the
68. How the AI safety technical landscape has changed in the last year, according to some practitioners - Effective Altruism Forum, consulté le avril 12, 2025, https://forum.effectivealtruism.org/posts/zsMnxJpeiiytDrAss/how-the-ai-safety-technical-landscape-has-changed-in-the
69. Model Organisms of Misalignment: The Case for a New Pillar of Alignment Research, consulté le avril 12, 2025, https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1
70. Clarifying "AI Alignment" — AI Alignment Forum, consulté le avril 12, 2025, https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment
71. Alignment 201 curriculum — EA Forum - Effective Altruism Forum, consulté le avril 12, 2025, https://forum.effectivealtruism.org/posts/GCfnaf5msCKiCngKs/alignment-201-curriculum
72. What Is AI Alignment? - IBM, consulté le avril 12, 2025, https://www.ibm.com/think/topics/ai-alignment
73. Clarifying “AI alignment”, consulté le avril 12, 2025, https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6
74. Paul Christiano: Current Work in AI Alignment - Effective Altruism, consulté le avril 12, 2025, https://www.effectivealtruism.org/articles/paul-christiano-current-work-in-ai-alignment
75. Don't Call It AI Alignment — EA Forum, consulté le avril 12, 2025, https://forum.effectivealtruism.org/posts/6aYfWyo9DKEheogf8/don-t-call-it-ai-alignment
76. AI alignment with humans... but with which humans? — EA Forum - Effective Altruism Forum, consulté le avril 12, 2025, https://forum.effectivealtruism.org/posts/DXuwsXsqGq5GtmsB3/ai-alignment-with-humans-but-with-which-humans
77. AI alignment - Wikipedia, consulté le avril 12, 2025, https://en.wikipedia.org/wiki/AI_alignment
78. [2310.19852] AI Alignment: A Comprehensive Survey - arXiv, consulté le avril 12, 2025, https://arxiv.org/abs/2310.19852
79. Key Concepts in AI Safety: An Overview | Center for Security and Emerging Technology, consulté le avril 12, 2025, https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-an-overview/
80. AI Safety for Everyone - arXiv, consulté le avril 12, 2025, https://arxiv.org/html/2502.09288v2
81. Defining alignment research — EA Forum - Effective Altruism Forum, consulté le avril 12, 2025, https://forum.effectivealtruism.org/posts/ajcQELstaSGYxdoRj/defining-alignment-research
82. Managing AI Risks in an Era of Rapid Progress, consulté le avril 12, 2025, https://managing-ai-risks.com/managing_ai_risks.pdf
83. arxiv.org, consulté le avril 12, 2025, https://arxiv.org/abs/2404.02675
84. arxiv.org, consulté le avril 12, 2025, https://arxiv.org/abs/2311.15936
85. arxiv.org, consulté le avril 12, 2025, https://arxiv.org/abs/2302.04844
86. arxiv.org, consulté le avril 12, 2025, https://arxiv.org/abs/2406.04313
87. Oversight for Frontier AI through a Know-Your-Customer Scheme for ..., consulté le avril 12, 2025, https://www.governance.ai/research-paper/oversight-for-frontier-ai-through-kyc-scheme-for-compute-providers
88. [2501.04952] Open Problems in Machine Unlearning for AI Safety - arXiv, consulté le avril 12, 2025, https://arxiv.org/abs/2501.04952
89. Deep Forgetting & Unlearning for Safely-Scoped LLMs - AI Alignment Forum, consulté le avril 12, 2025, https://www.alignmentforum.org/posts/mFAvspg4sXkrfZ7FA/deep-forgetting-and-unlearning-for-safely-scoped-llms
90. AI Alignment - Robustness, unlearning and control, consulté le avril 12, 2025, https://course.bluedot.org/home/alignment?session=5&scrollTo=resources
91. Supporting Trustworthy AI Through Machine Unlearning - PMC, consulté le avril 12, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC11390766/
92. The Rise of Machine Unlearning - Pecan AI, consulté le avril 12, 2025, https://www.pecan.ai/blog/the-rise-of-machine-unlearning/
93. Announcing the first Machine Unlearning Challenge - Google Research, consulté le avril 12, 2025, https://research.google/blog/announcing-the-first-machine-unlearning-challenge/
94. [2502.14828] Fundamental Limitations in Defending LLM Finetuning APIs - arXiv, consulté le avril 12, 2025, https://arxiv.org/abs/2502.14828
95. Fundamental Limitations in Defending LLM Finetuning APIs - arXiv, consulté le avril 12, 2025, https://arxiv.org/pdf/2502.14828
96. arxiv.org, consulté le avril 12, 2025, https://arxiv.org/abs/2305.15324
97. Esvelt, Gopal and Jeyapragasan NIST RFI, consulté le avril 12, 2025, https://www.nist.gov/document/ai-eo-14110-rfi-comments-securebio
98. Third-party evaluation to identify risks in LLMs' training data ..., consulté le avril 12, 2025, https://blog.openmined.org/third-party-evaluation-to-identify-risks-in-llms-training-data/
99. What Is eKYC (Electronic Know Your Customer)? - Identity.com, consulté le avril 12, 2025, https://www.identity.com/what-is-ekyc-understanding-electronic-know-your-customer/
100. What are KYC + AML checks? Overview - IDnow, consulté le avril 12, 2025, https://www.idnow.io/regulation/aml-kyc-overview/
101. Know Your Customer (KYC) | TRM Glossary - TRM Labs, consulté le avril 12, 2025, https://www.trmlabs.com/glossary/know-your-customer-kyc
102. What is Know Your Customer (KYC)? - Sanction Scanner, consulté le avril 12, 2025, https://www.sanctionscanner.com/knowledge-base/know-your-customer-kyc-46
103. Optimizing KYC onboarding for compliance & fraud prevention, consulté le avril 12, 2025, https://www.fraud.com/post/kyc-onboarding
104. Will AI make KYC impossible? - Eastnets, consulté le avril 12, 2025, https://www.eastnets.com/blog/will-ai-make-kyc-impossible
105. Will KYC become obsolete in the AI era? - Eastnets, consulté le avril 12, 2025, https://www.eastnets.com/blog/will-kyc-become-obsolete-in-the-ai-era
106. consulté le janvier 1, 1970, https://www.nature.com/articles/d41586-024-02180-5
107. AI Could Pose Pandemic-Scale Biosecurity Risks: Here's How to Make It Safer | RAND, consulté le avril 12, 2025, https://www.rand.org/pubs/external_publications/EP70827.html
108. AI could pose pandemic-scale biosecurity risks. Here's how to make it safer, consulté le avril 12, 2025, https://www.csnsf.org/ai-could-pose-pandemic-scale-biosecurity-risks-heres-how-to-make-it-safer/
109. Managing Risks from AI-Enabled Biological Tools | GovAI Blog, consulté le avril 12, 2025, https://www.governance.ai/post/managing-risks-from-ai-enabled-biological-tools
110. Global Conference on AI, Security and Ethics - UNIDIR, consulté le avril 12, 2025, https://unidir.org/wp-content/uploads/2025/04/AISE25_posters.pdf
111. Responsible AI in biotechnology: balancing discovery, innovation and biosecurity risks, consulté le avril 12, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC11835847/
112. consulté le janvier 1, 1970, https://helentoner.substack.com/p/nonproliferation-is-the-wrong-approach
113. My techno-optimism - Vitalik Buterin's website, consulté le avril 12, 2025, https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html
114. Links and short notes, 2025-03-18 - The Roots of Progress, consulté le avril 12, 2025, https://blog.rootsofprogress.org/links-and-short-notes-2025-03-18
115. Vitalik published a long article titled "d/acc: one year later," discussing AI safety and cryptocurrency applications - ChainCatcher, consulté le avril 12, 2025, https://www.chaincatcher.com/en/article/2160810
116. Vitalik's New Article: Decentralization Accelerationism One-Year, consulté le avril 12, 2025, https://www.bitget.com/news/detail/12560604474294
117. consulté le janvier 1, 1970, https://www.alignmentforum.org/posts/Cpip6KjbJd53sGBgs/access-to-powerful-ai-might-make-computer-security
118. An overview of areas of control work - AI Alignment Forum, consulté le avril 12, 2025, https://www.alignmentforum.org/posts/Eeo9NrXeotWuHCgQW/an-overview-of-areas-of-control-work
119. Computer Security & Cryptography tag - LessWrong 2.0 viewer, consulté le avril 12, 2025, https://www.greaterwrong.com/tag/computer-security-and-cryptography?sort=old
120. Access to powerful AI might make computer security radically easier - AI Alignment Forum, consulté le avril 12, 2025, https://www.alignmentforum.org/posts/2wxufQWK8rXcDGbyL/access-to-powerful-ai-might-make-computer-security-radically
121. Access to powerful AI might make computer security radically easier - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/2wxufQWK8rXcDGbyL/access-to-powerful-ai-might-make-computer-security-radically
122. Understanding Offensive AI vs. Defensive AI in Cybersecurity - Abnormal Security, consulté le avril 12, 2025, https://abnormalsecurity.com/blog/offensive-ai-defensive-ai
123. Counter AI Attacks with AI Defense - Palo Alto Networks, consulté le avril 12, 2025, https://www.paloaltonetworks.com/blog/2024/05/counter-with-ai-defense/
124. consulté le janvier 1, 1970, https://www.lesswrong.com/posts/LyrSSbDWSDomKiXoQ/open-source-ai-has-been-vital-for-alignment
125. Open source AI has been vital for alignment - Beren's Blog, consulté le avril 12, 2025, https://www.beren.io/2023-11-05-Open-source-AI-has-been-vital-for-alignment/
126. IAPS: Mapping Technical Safety Research at AI Companies - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/LmhpAyCn8Xpqfwvhb/iaps-mapping-technical-safety-research-at-ai-companies
127. arxiv.org, consulté le avril 12, 2025, https://arxiv.org/abs/2311.08105
128. Import AI 404: Scaling laws for distributed training; misalignment predictions made real; and Alibaba's good translation model, consulté le avril 12, 2025, https://jack-clark.net/2025/03/17/import-ai-404-scaling-laws-for-distributed-training-misalignment-predictions-made-real-and-alibabas-good-translation-model/
129. Import AI 398: DeepMind makes distributed training better; AI versus the Intelligence Community; and another Chinese reasoning model, consulté le avril 12, 2025, https://jack-clark.net/2025/02/03/import-ai-398-deepmind-makes-distributed-training-better-ai-versus-the-intelligence-community-and-another-chinese-reasoning-model/
130. Import AI 387: Overfitting vs reasoning; distributed training runs; and Facebook's new video models, consulté le avril 12, 2025, https://jack-clark.net/2024/10/14/import-ai-387-overfitting-vs-reasoning-distributed-training-runs-and-facebooks-new-video-models/
131. Import AI 367: Google's world-spanning model; breaking AI policy with evolution; $250k for alignment benchmarks, consulté le avril 12, 2025, https://jack-clark.net/2024/04/01/import-ai-367-googles-world-spanning-model-breaking-ai-policy-with-evolution-250k-for-alignment-benchmarks/
132. Open-Sourcing Highly Capable Foundation Models | GovAI, consulté le avril 12, 2025, https://www.governance.ai/research-paper/open-sourcing-highly-capable-foundation-models
133. arxiv.org, consulté le avril 12, 2025, https://arxiv.org/abs/2311.09227
134. Sayash Kapoor - cs.Princeton, consulté le avril 12, 2025, https://www.cs.princeton.edu/~sayashk/
135. Christoph Schuhmann on Open Source AI - The Inside View, consulté le avril 12, 2025, https://theinsideview.ai/christoph
136. arxiv.org, consulté le avril 12, 2025, https://arxiv.org/abs/2001.00463
137. Comments - On the Societal Impact of Open Foundation Models - AI Snake Oil, consulté le avril 12, 2025, https://www.aisnakeoil.com/p/on-the-societal-impact-of-open-foundation/comments
138. NTIA RFC - Open Foundation Models - Stanford HAI, consulté le avril 12, 2025, https://hai.stanford.edu/sites/default/files/2024-03/Response-NTIA-RFC-Open-Foundation-Models.pdf
139. WITSA Webinar On the Societal Impact of Open Foundation Models (April 3, 2024), consulté le avril 12, 2025, https://www.youtube.com/watch?v=_GGE67Ptua4
140. arxiv.org, consulté le avril 12, 2025, https://arxiv.org/abs/1908.09203
141. arxiv.org, consulté le avril 12, 2025, https://arxiv.org/abs/2303.09377
142. Map - NIST AIRC - National Institute of Standards and Technology, consulté le avril 12, 2025, https://airc.nist.gov/airmf-resources/playbook/map/
143. Detecting and Preventing AI-Based Phishing Attacks: 2024 Guide - Perception Point, consulté le avril 12, 2025, https://perception-point.io/guides/ai-security/detecting-and-preventing-ai-based-phishing-attacks-2024-guide/
144. How AI Is Used in Fraud Detection in 2025 - DataDome, consulté le avril 12, 2025, https://datadome.co/learning-center/ai-fraud-detection/
145. Protect Your Subscribers Against Scam Calls with AI-Powered SCAMBlock, consulté le avril 12, 2025, https://www.neuralt.com/news-insights/protect-your-subscribers-against-scam-calls-with-ai-powered-scamblock
146. Block Spam Calls (Robocalls) with AI and machine learning | SCAMBlock, consulté le avril 12, 2025, https://www.neuralt.com/news-insights/block-spam-calls-and-robocalls-with-ai-and-machine-learning-scamblock
147. Google: Real-time detection and protection against phone scams - Telefónica Tech, consulté le avril 12, 2025, https://telefonicatech.com/en/blog/real-time-detection-and-protection-against-phone-scams
148. consulté le janvier 1, 1970, https://fr.euronews.com/next/2024/03/08/decouvrez-daisy-le-chatbot-mamie-qui-fait-perdre-du-temps-aux-fraudeurs-au-telephone
149. Meet dAIsy, the scam-fighting AI bot - O2, consulté le avril 12, 2025, https://www.o2.co.uk/inspiration/the-drop/meet-daisy-the-scam-fighting-ai-bot
150. AI Scambaiters: O2 creates AI Granny to waste scammers' time - YouTube, consulté le avril 12, 2025, https://www.youtube.com/watch?v=RV_SdCfZ-0s
151. Jolly Roger Telephone | Revenge Has Never Been So Sweet, consulté le avril 12, 2025, https://jollyrogertelephone.com/
152. Phone network employs AI "grandmother" to waste scammers' time with meandering conversations | Scambaiting, Abe Simpson-style : r/technology - Reddit, consulté le avril 12, 2025, https://www.reddit.com/r/technology/comments/1gr3cv3/phone_network_employs_ai_grandmother_to_waste/
153. AI Safety Strategies Landscape - AI Alignment Forum, consulté le avril 12, 2025, https://www.alignmentforum.org/posts/RzsXRbk2ETNqjhsma/ai-safety-strategies-landscape
154. AI is easy to control – AI Optimism, consulté le avril 12, 2025, https://optimists.ai/2023/11/28/ai-is-easy-to-control/
155. Truth is Universal: Robust Detection of Lies in LLMs - OpenReview, consulté le avril 12, 2025, [https://openreview.net/forum?id=1Fc2Xa2cDK&referrer=%5Bthe%20profile%20of%20Fred%20A.%20Hamprecht%5D(%2Fprofile%3Fid%3D~Fred_A._Hamprecht1)](https://openreview.net/forum?id=1Fc2Xa2cDK&referrer=%5Bthe+profile+of+Fred+A.+Hamprecht%5D(/profile?id%3D~Fred_A._Hamprecht1))
156. Truth is Universal: Robust Detection of Lies in LLMs - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/72vpkRRvoPHKi48fi/truth-is-universal-robust-detection-of-lies-in-llms-3
157. Truth is Universal: Robust Detection of Lies in LLMs - arXiv, consulté le avril 12, 2025, https://arxiv.org/html/2407.12831v2
158. Truth is Universal: Robust Detection of Lies in LLMs - arXiv, consulté le avril 12, 2025, https://arxiv.org/html/2407.12831v1
159. Truth is Universal: Robust Detection of Lies in LLMs - NIPS papers, consulté le avril 12, 2025, https://papers.nips.cc/paper_files/paper/2024/file/f9f54762cbb4fe4dbffdd4f792c31221-Paper-Conference.pdf
160. How might we safely pass the buck to AI? — LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/TTFsKxQThrqgWeXYJ/how-might-we-safely-pass-the-buck-to-ai
161. How might we safely pass the buck to AI? - AI Alignment Forum, consulté le avril 12, 2025, https://www.alignmentforum.org/posts/TTFsKxQThrqgWeXYJ/how-might-we-safely-pass-the-buck-to-ai
162. A sketch of an AI control safety case - arXiv, consulté le avril 12, 2025, https://arxiv.org/html/2501.17315v1
163. How will we update about scheming? - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/aEguDPoCzt3287CCD/how-will-we-update-about-scheming
164. Frontier Models are Capable of In-context Scheming - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/8gy7c8GAPkuu6wTiX/frontier-models-are-capable-of-in-context-scheming
165. consulté le janvier 1, 1970, https://www.lesswrong.com/posts/C4ivL5gSDsyJ49fg9/how-will-we-update-about-scheming
166. Frontier Models are Capable of In-context Scheming - AI Alignment Forum, consulté le avril 12, 2025, https://www.alignmentforum.org/posts/8gy7c8GAPkuu6wTiX/frontier-models-are-capable-of-in-context-scheming
167. Towards evaluations-based safety cases for AI scheming - arXiv, consulté le avril 12, 2025, https://arxiv.org/abs/2411.03336
168. An overview of control measures - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/G8WwLmcGFa4H6Ld9d/an-overview-of-control-measures
169. Detecting misbehavior in frontier reasoning models | OpenAI, consulté le avril 12, 2025, https://openai.com/index/chain-of-thought-monitoring/
170. Reasoning models don't always say what they think \ Anthropic, consulté le avril 12, 2025, https://www.anthropic.com/research/reasoning-models-dont-say-think
171. Safety Case Template for Frontier AI: A Cyber Inability Argument ..., consulté le avril 12, 2025, https://www.governance.ai/research-paper/safety-case-template-for-frontier-ai-a-cyber-inability-argument
172. How to evaluate control measures for LLM agents? A trajectory from today to superintelligence - arXiv, consulté le avril 12, 2025, https://arxiv.org/html/2504.05259v1
173. (PDF) A sketch of an AI control safety case - ResearchGate, consulté le avril 12, 2025, https://www.researchgate.net/publication/388494787_A_sketch_of_an_AI_control_safety_case
174. The BIG Argument for AI Safety Cases - arXiv, consulté le avril 12, 2025, https://arxiv.org/html/2503.11705v1
175. arxiv.org, consulté le avril 12, 2025, https://arxiv.org/abs/2406.15371
176. Paul Christiano - Preventing AI Takeover - Dwarkesh Podcast, consulté le avril 12, 2025, https://www.dwarkeshpatel.com/p/paul-christiano
177. Superintelligence Strategy: Expert Version - arXiv, consulté le avril 12, 2025, https://arxiv.org/pdf/2503.05628
178. Holden Karnofsky - Transformative AI & Most Important Century, consulté le avril 12, 2025, https://www.dwarkeshpatel.com/p/holden-karnofsky
179. Life 3.0 Book Summary by Max Tegmark - Shortform, consulté le avril 12, 2025, https://www.shortform.com/summary/life-3-0-summary-max-tegmark
180. Life 3.0 Summary and Study Guide - SuperSummary, consulté le avril 12, 2025, https://www.supersummary.com/life-3-0/summary/
181. Life 3.0 Summary of Key Ideas and Review | Max Tegmark - Blinkist, consulté le avril 12, 2025, https://www.blinkist.com/en/books/life-3-dot-0-en
182. Book Summary – Life 3.0: Being Human in the Age of Artificial Intelligence - Readingraphics, consulté le avril 12, 2025, https://readingraphics.com/book-summary-life-3-0/
183. A Worthy Successor - The Purpose of AGI - Dan Faggella, consulté le avril 12, 2025, https://danfaggella.com/worthy/
184. Oxford researchers awarded ARIA funding to develop safety-first AI, consulté le avril 12, 2025, https://www.ox.ac.uk/news/2025-04-10-oxford-researchers-awarded-aria-funding-develop-safety-first-ai
185. In response to critiques of Guaranteed Safe AI - AI Alignment Forum, consulté le avril 12, 2025, https://www.alignmentforum.org/posts/DZuBHHKao6jsDDreH/in-response-to-critiques-of-guaranteed-safe-ai
186. Safeguarded AI - Advanced Research and Invention Agency (ARIA), consulté le avril 12, 2025, https://www.aria.org.uk/safeguarded-ai/
187. Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems, consulté le avril 12, 2025, https://arxiv.org/html/2405.06624v2
188. Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems, consulté le avril 12, 2025, https://arxiv.org/abs/2405.06624
189. In response to critiques of Guaranteed Safe AI - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/DZuBHHKao6jsDDreH/in-response-to-critiques-of-guaranteed-safe-ai
190. Limitations on Formal Verification for AI Safety - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/B2bg677TaS4cmDPzL/limitations-on-formal-verification-for-ai-safety
191. Safeguarded AI Summary - Atlas Computing, consulté le avril 12, 2025, https://atlascomputing.org/safeguarded-ai-summary.pdf
192. Provably Safe AI: Worldview and Projects - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/P8XcbnYi7ooB2KR2j/provably-safe-ai-worldview-and-projects
193. On the Rationality of Deterring ASI - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/XsYQyBgm8eKjd3Sqw/on-the-rationality-of-deterring-asi
194. consulté le janvier 1, 1970, https://www.lesswrong.com/posts/j5v4p5kpfS75mP7HN/on-the-rationality-of-deterring-asi
195. Share AI Safety Ideas: Both Crazy and Not. №2 - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/cYseakJxjsicHSSJC/share-ai-safety-ideas-both-crazy-and-not-2
196. consulté le janvier 1, 1970, https://www.lesswrong.com/posts/Hw65ggm27SAJzHnuY/see-new-edits-no-you-need-to-write-clearer
197. [SEE NEW EDITS] No, *You* Need to Write Clearer - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/mLubC65xXekk5tkug/see-new-edits-no-you-need-to-write-clearer
198. The Nonlinear Library Podcast Republic, consulté le avril 12, 2025, https://www.podcastrepublic.net/podcast/1587343144
199. Better without AI: Table of contents, consulté le avril 12, 2025, https://betterwithout.ai/
200. consulté le janvier 1, 1970, https://keepthefuturehuman.com/essay/
201. Keep the Future Human: Why and How We Should Close the Gates to AGI and Superintelligence, and What We Should Build Instead - arXiv, consulté le avril 12, 2025, https://arxiv.org/pdf/2311.09452
202. Essay - Keep The Future Human, consulté le avril 12, 2025, https://keepthefuturehuman.ai/essay/
203. The Missing Piece: Why We Need a Grand Strategy for AI - Effective Altruism Forum, consulté le avril 12, 2025, https://forum.effectivealtruism.org/posts/4JmB7vaLZodFWDD3n/the-missing-piece-why-we-need-a-grand-strategy-for-ai
204. Upcoming Seminars | The Program in Arms Control & Domestic and International Security, consulté le avril 12, 2025, https://acdis.illinois.edu/news-events/events/upcoming-seminars
205. Now is the time to create a CERN for AI - Confederation of Laboratories for Artificial Intelligence Research in Europe, consulté le avril 12, 2025, https://cairne.eu/wp-content/uploads/2025/01/CAIRNE-open-letter-Jan-2025.pdf
206. EU science advisers back call for a 'CERN for AI' to aid research, consulté le avril 12, 2025, https://sciencebusiness.net/news/ai/eu-science-advisers-back-call-cern-ai-aid-research
207. Building CERN for AI - An institutional blueprint - Centre for Future Generations, consulté le avril 12, 2025, https://cfg.eu/building-cern-for-ai/
208. How the “CERN for AI” idea could become Europe's path to global leadership in AI, consulté le avril 12, 2025, https://cfg.eu/cern-for-ai-press-release/
209. Artificial intelligence and the challenge for global governance | 02 A 'CERN for AI' – what might an international AI research organization address? - Chatham House, consulté le avril 12, 2025, https://www.chathamhouse.org/2024/06/artificial-intelligence-and-challenge-global-governance/02-cern-ai-what-might-international
210. IASEAI Issues Call to Action for Lawmakers, Academics, and the Public Ahead of AI Summit in Paris, consulté le avril 12, 2025, https://www.iaseai.org/conference/statement
211. IDAIS-Beijing - International Dialogues on AI Safety, consulté le avril 12, 2025, https://idais.ai/dialogue/idais-beijing/
212. AI red lines: the opportunities and challenges of setting limits - The World Economic Forum, consulté le avril 12, 2025, https://www.weforum.org/stories/2025/03/ai-red-lines-uses-behaviours/
213. International Dialogues on AI Safety - International Dialogues on AI Safety, consulté le avril 12, 2025, https://idais.ai/
214. The AI Red Line Challenge | TechPolicy.Press, consulté le avril 12, 2025, https://www.techpolicy.press/the-ai-red-line-challenge/
215. AI Researchers In West, China Identify AI 'Red Lines' | Silicon UK, consulté le avril 12, 2025, https://www.silicon.co.uk/e-innovation/artificial-intelligence/ai-red-lines-555036
216. Scientists Call for Global AI Safety Preparedness to Avert Catastrophic Risks - FAR.AI, consulté le avril 12, 2025, https://far.ai/post/2024-09-idais-venice/
217. Historic first as companies spanning North America, Asia, Europe and Middle East agree safety commitments on development of AI - GOV.UK, consulté le avril 12, 2025, https://www.gov.uk/government/news/historic-first-as-companies-spanning-north-america-asia-europe-and-middle-east-agree-safety-commitments-on-development-of-ai
218. Google, Meta, Microsoft, Others Agree on AI 'Red Lines' - Asia Financial, consulté le avril 12, 2025, https://www.asiafinancial.com/google-meta-microsoft-others-agree-on-ai-red-lines
219. If-Then Commitments for AI Risk Reduction | Carnegie Endowment for International Peace, consulté le avril 12, 2025, https://carnegieendowment.org/research/2024/09/if-then-commitments-for-ai-risk-reduction?center=russia-eurasia
220. LW - If-Then Commitments for AI Risk Reduction ... - Apple Podcasts, consulté le avril 12, 2025, https://podcasts.apple.com/at/podcast/lw-if-then-commitments-for-ai-risk-reduction-by-holden/id1587343144?i=1000669513422&l=en-GB
221. Our proposal in TIME: a Conditional AI Safety Treaty - Existential Risk Observatory, consulté le avril 12, 2025, https://www.existentialriskobservatory.org/ai/our-proposal-in-time-a-conditional-ai-safety-treaty/
222. There Is a Solution to AI's Existential Risk Problem: A Conditional AI Safety Treaty - Reddit, consulté le avril 12, 2025, https://www.reddit.com/r/Futurology/comments/1gsot6f/there_is_a_solution_to_ais_existential_risk/
223. There Is a Solution to AI's Existential Risk Problem | TIME, consulté le avril 12, 2025, https://time.com/7171432/conditional-ai-safety-treaty-trump/
224. arxiv.org, consulté le avril 12, 2025, https://arxiv.org/abs/2406.14713
225. 'Most dangerous technology ever': Protesters urge AI pause : r/Futurology - Reddit, consulté le avril 12, 2025, https://www.reddit.com/r/Futurology/comments/1ilfumo/most_dangerous_technology_ever_protesters_urge_ai/
226. Pause For Thought: The AI Pause Debate — EA Forum - Effective Altruism Forum, consulté le avril 12, 2025, https://forum.effectivealtruism.org/posts/7WfMYzLfcTyDtD6Gn/pause-for-thought-the-ai-pause-debate
227. An AI Pause Is Humanity's Best Bet For Preventing Extinction | TIME, consulté le avril 12, 2025, https://time.com/6295879/ai-pause-is-humanitys-best-bet-for-preventing-extinction/
228. Call for proposed designs for global institutions governing AI, consulté le avril 12, 2025, https://futureoflife.org/grant-program/global-institutions-governing-ai/
229. Ethical and Safe AI Development: Corporate Governance is the Missing Piece, consulté le avril 12, 2025, https://www.oxford-aiethics.ox.ac.uk/blog/Ethical-and-Safe-AI-Development-Corporate-Governance-is-the-Missing-Piece
230. Ethical and Safe AI Development: Corporate Governance is the Missing Piece, consulté le avril 12, 2025, https://isabelleferreras.net/ethical-and-safe-ai-development-corporate-governance-is-the-missing-piece/
231. Why Create an AI Whistleblower Policy for Compliance? | CSA - Cloud Security Alliance, consulté le avril 12, 2025, https://cloudsecurityalliance.org/articles/why-you-should-have-a-whistleblower-policy-for-ai
232. AI workers call for greater transparency and whistleblower protection - National Technology, consulté le avril 12, 2025, https://nationaltechnology.co.uk/AI_Workers_Call_For_Greater_Transparency_And_Whistleblower_Protection.php
233. Implementing an Effective Whistleblower System: Key Benefits and Best Practices, consulté le avril 12, 2025, https://www.dataguard.com/blog/implementing-an-effective-whistleblower-system/
234. Whistleblower Protections: Laws and Safeguards Against Retaliation | FaceUp Blog, consulté le avril 12, 2025, https://www.faceup.com/en/blog/whistleblower-protection
235. Despite Regulation Lag, AI Whistleblowers Have Protections - Katz Banks Kumin, consulté le avril 12, 2025, https://katzbanks.com/wp-content/uploads/KBK-Law360-Despite-Regulation-Lag-AI-Whistleblowers-Have-Protections.pdf
236. AI Further Regulated for Consumer Safety in CA - JDP, consulté le avril 12, 2025, https://www.jdp.com/blog/ai-further-regulated-for-consumer-safety-in-ca/
237. arxiv.org, consulté le avril 12, 2025, https://arxiv.org/abs/2303.11341
238. Secure, Governable Chips | CNAS, consulté le avril 12, 2025, https://www.cnas.org/publications/reports/secure-governable-chips
239. Governing Through the Cloud: The Intermediary… | Oxford Martin ..., consulté le avril 12, 2025, https://www.oxfordmartin.ox.ac.uk/publications/governing-through-the-cloud-the-intermediary-role-of-compute-providers-in-ai-regulation
240. arxiv.org, consulté le avril 12, 2025, https://arxiv.org/abs/2407.07852
241. consulté le janvier 1, 1970, https://newsletter.tolgabilge.com/p/two-years-of-ai-politics-past-present
242. Managing extreme AI risks amid rapid progress - arXiv, consulté le avril 12, 2025, https://arxiv.org/pdf/2310.17688
243. Big Tech Accountability Depends on Better Protection for Whistleblowers - The Signals Network, consulté le avril 12, 2025, https://thesignalsnetwork.org/wp-content/uploads/2024/03/Big-Tech-Accountability-Depends-on-Better-Protection-for-Whistleblowers.pdf
244. Ensuring Safe & Secure AI Adoption - A Guide for HR Professionals - Cerium Networks, consulté le avril 12, 2025, https://ceriumnetworks.com/ensuring-safe-secure-ai-adoption-a-guide-for-hr-professionals/
245. The complete guide to AI safety in the workplace - Protex AI, consulté le avril 12, 2025, https://www.protex.ai/guides/the-complete-guide-to-ai-safety-in-the-workplace
246. Building Trust in AI: Security and Risks in Highly Regulated Industries - InfoQ, consulté le avril 12, 2025, https://www.infoq.com/articles/building-trust-ai/
247. Managing Data Security and Privacy Risks in Enterprise AI | Frost Brown Todd, consulté le avril 12, 2025, https://frostbrowntodd.com/managing-data-security-and-privacy-risks-in-enterprise-ai/
248. AI Data Security: Complete Guide & Best Practices - BigID, consulté le avril 12, 2025, https://bigid.com/blog/ai-data-security/
249. Your AI security guide: Benefits and best practices - Cohere, consulté le avril 12, 2025, https://cohere.com/blog/ai-security
250. AI Cybersecurity Best Practices: Meeting a Double-Edged Challenge - Ivanti, consulté le avril 12, 2025, https://www.ivanti.com/blog/ai-cybersecurity-best-practices-meeting-a-double-edged-challenge
251. Anthropic achieves ISO 42001 certification for responsible AI ..., consulté le avril 12, 2025, https://www.anthropic.com/news/anthropic-achieves-iso-42001-certification-for-responsible-ai
252. AI RMF PLAYBOOK - Privacy + Security Academy, consulté le avril 12, 2025, https://www.privacysecurityacademy.com/wp-content/uploads/2024/10/AI_RMF_Playbook-1.pdf
253. System Cards for AI-Based Decision-Making for Public Policy | Montreal AI Ethics Institute, consulté le avril 12, 2025, https://montrealethics.ai/system-cards-for-ai-based-decision-making-for-public-policy/
254. Managing AI Risks in an Era of Rapid Progress — Institute for AI ..., consulté le avril 12, 2025, https://www.iaps.ai/research/managing-ai-risks
255. (PDF) Letter to the Editor of Clinical Orthopaedics & Related Research | Can Generative Artificial Intelligence and Large Language Models be a Catalyst for Value-based Healthcare? - ResearchGate, consulté le avril 12, 2025, https://www.researchgate.net/publication/375416888_Letter_to_the_Editor_of_Clinical_Orthopaedics_Related_Research_Can_Generative_Artificial_Intelligence_and_Large_Language_Models_be_a_Catalyst_for_Value-based_Healthcare
256. Taking control: Policies to address extinction risks from advanced AI - arXiv, consulté le avril 12, 2025, https://arxiv.org/pdf/2310.20563
257. arxiv.org, consulté le avril 12, 2025, https://arxiv.org/abs/2212.08364
258. arxiv.org, consulté le avril 12, 2025, https://arxiv.org/abs/2406.12137
259. How much to update on recent AI governance moves? - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/HAmH8bzDNrHDwmcPb/how-much-to-update-on-recent-ai-governance-moves
260. Against Almost Every Theory of Impact of Interpretability - AI Alignment Forum, consulté le avril 12, 2025, https://www.alignmentforum.org/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1
261. Why do Experts Disagree on Existential Risk and P (doom)? A ..., consulté le avril 12, 2025, https://arxiv.org/abs/2502.14870
262. Results of an Adversarial Collaboration on AI Risk — Forecasting ..., consulté le avril 12, 2025, https://forecastingresearch.org/news/ai-adversarial-collaboration
263. consulté le janvier 1, 1970, https://www.lesswrong.com/posts/wQwkz4Db4X9zHJh8v/modeling-transformative-ai-risk-mtair
264. Modeling Transformative AI Risks (MTAIR) Project - Summary Report - Semantic Scholar, consulté le avril 12, 2025, https://www.semanticscholar.org/paper/2e031cfd6c595ae9e53eb766e35a96422e6ed801
265. Modeling Risks From Learned Optimization - AI Alignment Forum, consulté le avril 12, 2025, https://www.alignmentforum.org/posts/T9oFjteStcE2ijCJi/modeling-risks-from-learned-optimization
266. The Direct Institutional Plan | ControlAI, consulté le avril 12, 2025, https://controlai.com/dip
267. arxiv.org, consulté le avril 12, 2025, https://arxiv.org/abs/1912.12835
268. Holly Elmore on AI Pause Advocacy - The Inside View, consulté le avril 12, 2025, https://theinsideview.ai/holly
269. Americans are increasingly skeptical about AI's effects | YouGov, consulté le avril 12, 2025, https://today.yougov.com/technology/articles/51803-americans-increasingly-skeptical-about-ai-artificial-intelligence-effects-poll
270. www.existentialriskobservatory.org, consulté le avril 12, 2025, https://www.existentialriskobservatory.org/papers_and_reports/Trends%20in%20Public%20Attitude%20Towards%20Existential%20Risk%20And%20Artificial%20Intelligence.pdf
271. Resources to Take Action | AI Safety, Ethics, and Society Course, consulté le avril 12, 2025, https://www.aisafetybook.com/take-action
272. Resources – BlueDot Impact - AI Safety Fundamentals, consulté le avril 12, 2025, https://aisafetyfundamentals.com/resources/
273. List of AI safety newsletters and other resources - Effective Altruism Forum, consulté le avril 12, 2025, https://forum.effectivealtruism.org/posts/hsmh4fD8Dbkzvdehk/list-of-ai-safety-newsletters-and-other-resources
274. Trustworthy and Safety AI Resources - Tuan-Anh Bui, consulté le avril 12, 2025, https://tuananhbui89.github.io/blog/2024/safeai-resources/
275. AI Alignment Forum, consulté le avril 12, 2025, https://www.alignmentforum.org/
276. AI: Practical Advice for the Worried - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/CvfZrrEokjCu3XHXp/ai-practical-advice-for-the-worried
277. Research | GovAI - Centre for the Governance of AI, consulté le avril 12, 2025, https://www.governance.ai/research
278. Bio-Risk | Center for Security and Emerging Technology - CSET, consulté le avril 12, 2025, https://cset.georgetown.edu/research-topic/bio-risk/
279. Biotechnology - RAND, consulté le avril 12, 2025, https://www.rand.org/topics/biotechnology.html
280. Introductory Resources on AI Safety Research - Future of Life Institute, consulté le avril 12, 2025, https://futureoflife.org/recent-news/introductory-resources-on-ai-safety-research/
281. Life 3.0 - Future of Life Institute, consulté le avril 12, 2025, https://futureoflife.org/resource/life-3-0-being-human-in-the-age-of-artificial-intelligence/
282. Monthly Overload of Effective Altruism | David Nash | Substack, consulté le avril 12, 2025, https://moea.substack.com/
283. consulté le janvier 1, 1970, https://www.aiassurance.tech/
284. Slowing AI: Foundations - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/MoLLqFtMup39PCsaG/slowing-ai-foundations
285. Preparing for Accelerated AGI Timelines, consulté le avril 12, 2025, https://framerusercontent.com/assets/RpOeVR5NtCKpwU4DXGYHumk5LEQ.pdf
286. consulté le janvier 1, 1970, https://www.thezvi.com/p/the-big-nonprofits-post
287. Conversation with Paul Christiano - AI Impacts, consulté le avril 12, 2025, https://aiimpacts.org/conversation-with-paul-christiano/
288. consulté le janvier 1, 1970, https://far.ai/vienna-workshop-2024/
289. Natural Abstractions: Key claims, Theorems, and Critiques - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/gvzW46Z3BsaZsLc25/natural-abstractions-key-claims-theorems-and-critiques-1
290. What is John Wentworth's research agenda? - AISafety.info, consulté le avril 12, 2025, https://aisafety.info/questions/8378/What-is-John-Wentworth's-research-agenda
291. Testing The Natural Abstraction Hypothesis: Project Intro - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro
292. How might the natural abstraction hypothesis lead to "alignment by default"? - AISafety.info, consulté le avril 12, 2025, https://aisafety.info/questions/8AV4/How-might-the-natural-abstraction-hypothesis-lead-to-%22alignment-by-default%22
293. The Natural Abstraction Hypothesis: Implications and Evidence - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/Fut8dtFsBYRz8atFF/the-natural-abstraction-hypothesis-implications-and-evidence
294. If Wentworth is right about natural abstractions, it would be bad for alignment - GreaterWrong, consulté le avril 12, 2025, https://www.greaterwrong.com/posts/nJHXQWCSByS4SxfQz/if-wentworth-is-right-about-natural-abstractions-it-would-be
295. Natural abstractions are observer-dependent: a conversation with John Wentworth - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/CJjT8GMitsnKc2wgG/natural-abstractions-are-observer-dependent-a-conversation-1
296. Natural abstractions are observer-dependent: a conversation with John Wentworth - AI Alignment Forum, consulté le avril 12, 2025, https://www.alignmentforum.org/posts/CJjT8GMitsnKc2wgG/natural-abstractions-are-observer-dependent-a-conversation-1
297. Timaeus in 2024 - AI Alignment Forum, consulté le avril 12, 2025, https://www.alignmentforum.org/posts/gGAXSfQaiGBCwBJH5/timaeus-in-2024
298. LessWrong posts by zvi, consulté le avril 12, 2025, https://podcast.lesswrong.com/users/zvi.rss
299. AI Safety - 7 months of discussion in 17 minutes - LessWrong, consulté le avril 12, 2025, https://www.lesswrong.com/posts/voehWNwi62JLCXEoG/ai-safety-7-months-of-discussion-in-17-minutes
300. EPIC Comments to NIST on Managing the Risks of Misuse with AI Foundation Models, consulté le avril 12, 2025, https://epic.org/documents/epic-comments-to-nist-on-managing-the-risks-of-misuse-with-ai-foundation-models/
301. Mapping AI Safety Research: An Open-Source Knowledge Graph, consulté le avril 12, 2025, https://apartresearch.com/news/mapping-ai-safety-research-an-open-source-knowledge-graph
302. What's the short timeline plan? - AI Alignment Forum, consulté le avril 12, 2025, https://www.alignmentforum.org/posts/bb5Tnjdrptu89rcyY/what-s-the-short-timeline-plan